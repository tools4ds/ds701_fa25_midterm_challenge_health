{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3dda76",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"ds701_midterm25_notebook.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7479ca6d",
   "metadata": {},
   "source": [
    "## Stroke Risk Prediction - DS701 Midterm Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv(\"strokeX.csv\")\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# Store the original columns\n",
    "original_columns = df.columns.tolist()\n",
    "\n",
    "# Unique patient record\n",
    "print(f\"Unique patients: {df.shape[0]} (1 record per patient assumed)\")\n",
    "\n",
    "# Basic descriptive stats\n",
    "df.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2753f39e",
   "metadata": {},
   "source": [
    "### Part 1 - Exploratory Feature Analysis & Risk Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52f94d7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q1.1 Age-Normalized Risk Index (ANRI) (5 points)\n",
    "\n",
    "The **Age-Normalized Risk Index (ANRI)** identifies patients whose stroke risk is unusually high for their age.  \n",
    "A higher ANRI indicates a higher stroke risk relative to age.\n",
    "\n",
    "*Hint: ANRI is calculated by dividing each patient’s `stroke_risk_pct` by their `age`.*\n",
    "\n",
    "**Tasks:**\n",
    "1. Compute a new column `ANRI` (handle divide-by-zero safely).  \n",
    "2. Plot the distribution of ANRI using a **histogram** (with KDE).   \n",
    "3. Show the **top 10 patients** with the highest ANRI values and briefly interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ef545a",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_anri(df):\n",
    "    \"\"\"\n",
    "    Compute and visualize the Age-Normalized Risk Index (ANRI)\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "df = compute_anri(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ca026",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q1.2 Chronic Condition Score (CCS) (5 points)\n",
    "\n",
    "The **Chronic Condition Score (CCS)** represents the number of chronic cardiovascular conditions a patient has, specifically high blood pressure and irregular heartbeat.  \n",
    "A higher CCS indicates greater chronic disease burden.\n",
    "\n",
    "*Hint: CCS is the sum of the binary columns `high_bp` and `irregular_heartbeat`.*\n",
    "\n",
    "**Tasks:**\n",
    "1. Add a new column `CCS` to the DataFrame.\n",
    "2. Compute the **average stroke risk percentage** for each CCS level (0, 1, 2).  \n",
    "3. Visualize the results using a **bar plot**, and interpret how stroke risk changes with higher CCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad981b10",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_ccs(df):\n",
    "    \"\"\"\n",
    "    Compute and visualize the Chronic Condition Score (CCS)\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "ccs_summary = compute_ccs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af1c45c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q1.3 Symptom Burden Index (SBI) (5 points)\n",
    "\n",
    "The **Symptom Burden Index (SBI)** quantifies the total number of symptoms reported by each patient.  \n",
    "A higher SBI indicates a greater overall symptom load, which is expected to relate to higher stroke risk.\n",
    "\n",
    "*Hint: SBI is the sum of all symptom indicator columns (values 0 or 1).*\n",
    "\n",
    "**Tasks:**\n",
    "1. Compute the total `SBI` for each patient using all symptom columns. Categorize patients into three groups based on SBI - *Low (0–3)*; *Moderate (4–6)*; *High (7+)*\n",
    "2. Analyze how the **average stroke risk percentage** changes across these SBI groups \n",
    "3. Compute the **correlation** between `SBI` and `stroke_risk_pct`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131aed0c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_sbi(df):\n",
    "    \"\"\"\n",
    "    Compute the Symptom Burden Index (SBI)\n",
    "    \"\"\"\n",
    "   \n",
    "    ...\n",
    "\n",
    "sbi_summary = compute_sbi(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a685a33",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q1.4 Symptom Predictive Power using Mutual Information (MI) (5 points)\n",
    "\n",
    "**Mutual Information (MI)** measures how informative a feature is for predicting a target variable.  \n",
    "Here, it shows how strongly each symptom relates to the stroke risk label (`at_risk`).\n",
    "\n",
    "*Hint: Use *`mutual_info_classif`* from *`sklearn.feature_selection`* to compute MI scores.*\n",
    "\n",
    "**Tasks:**\n",
    "1. Compute the 'MI' score between each symptom and `at_risk`.  \n",
    "2. Sort symptoms by MI score and list the **Top 10** most predictive ones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa4f8b",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_mi(df):\n",
    "    \"\"\"\n",
    "    Compute the Mutual Information (MI) score\n",
    "    \"\"\"\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "    ...\n",
    "\n",
    "mi_summary = compute_mi(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63a4a4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q1.5 Age-Adjusted Risk Z-Scores (AARZ) (5 points)\n",
    "\n",
    "The **Age-Adjusted Risk Z-Score (AARZ)** compares a patient’s stroke risk to others in the same age group.  \n",
    "It highlights patients whose risk levels are unusually high or low relative to their peers.\n",
    "\n",
    "*Hint: For each 10-year age group, calculate how many standard deviations a patient’s stroke risk percentage is above or below the group’s mean value.*\n",
    "\n",
    "**Tasks:**\n",
    "1. Create 10-year age bins, within each age group, compute the *Z-score* for stroke risk (`stroke_risk_pct`).  \n",
    "2. Identify the **Top 5 patients** with the highest Z-scores (highest relative risk).  \n",
    "3. For **each age group**, list the **Top 2 patients** with the highest Z-scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a205d",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_aarz(df):\n",
    "    \"\"\"\n",
    "    Compute Age-Adjusted Risk Z-Scores (AARZ)\n",
    "    \"\"\"\n",
    "\n",
    "    ...\n",
    "    \n",
    "top5_outliers, top_by_group = compute_aarz(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1249d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q1.6 Risk Consistency Index (RCI) (5 points)\n",
    "\n",
    "The **Risk Consistency Index (RCI)** measures how well the continuous stroke risk scores align with the binary label `at_risk`.  \n",
    "It captures how distinctly the two groups: *at risk* and *not at risk*, differ in their average stroke risk percentage.\n",
    "Higher RCI values indicate stronger consistency between risk scores and the `at_risk` label.\n",
    "\n",
    "*Hint: RCI is calculated by taking the absolute difference between the group means and dividing it by the pooled standard deviation.*  \n",
    "\n",
    "**Tasks:** \n",
    "1. Compute the mean and standard deviation of `stroke_risk_pct` for both groups and \n",
    "2. Calculate the `RCI`.\n",
    "2. Interpret whether stroke risk percentages are consistent with the binary classification labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7665e",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_rci(df):\n",
    "    \"\"\"\n",
    "    Compute the Risk Consistency Index (RCI) \n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "RCI = compute_rci(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef6a52",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q1.7 Composite Health Risk Index (CHRI) (5 points)\n",
    "\n",
    "The **Composite Health Risk Index (CHRI)** combines multiple cardiovascular indicators into a single, weighted risk score.  \n",
    "It integrates *high blood pressure*, *irregular heartbeat*, and *age-normalized risk (ANRI)* to capture an individual’s overall health vulnerability.\n",
    "\n",
    "*Hint:* Compute CHRI using the weighted formula:  *CHRI = 0.4 * High Blood Pressure + 0.4 * Irregular Heartbeat + 0.2 * ANRI*\n",
    "\n",
    "**Tasks:**\n",
    "1. Calculate the **Composite Health Risk Index (CHRI)** for each patient.   \n",
    "2. Find the **Top 5 patients** with the highest CHRI values.\n",
    "3.   Compute correlations between `CHRI` and both `stroke_risk_pct` and `at_risk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b34e81",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_chri(df):\n",
    "    \"\"\"\n",
    "    Compute the Composite Health Risk Index (CHRI) \n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "df, top_chri, corr_chri_risk, corr_chri_label = compute_chri(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c8bafe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q1.8 Risk Stratification Buckets (5 points)\n",
    "\n",
    "The **Risk Stratification Buckets** divide patients into *Low*, *Moderate*, and *High* risk groups based on their normalized stroke risk percentage.  \n",
    "This helps compare how *age*, *symptom burden (SBI)*, and *chronic conditions (CCS)* vary across different risk levels.\n",
    "\n",
    "*Hint: Normalize `stroke_risk_pct` by dividing by 100, then assign each patient to a risk category.*\n",
    "\n",
    "**Tasks:**\n",
    "1. Categorize patients into three buckets — *Low (0–0.3)*, *Moderate (0.3–0.7)*, *High (0.7–1.0)*. For each group, compute the average **age**, **SBI**, and **CCS**.  \n",
    "2. Visualize these averages using a bar plot with a log scale on the y-axis and interpret how patient characteristics differ by risk level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0153e0",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_risk_buckets(df):\n",
    "    \"\"\"\n",
    "    Create risk stratification buckets based on normalized stroke risk percentage\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "bucket_summary = compute_risk_buckets(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef347a9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newly Created Columns\n",
    "current_columns = df.columns.tolist()\n",
    "new_columns = [col for col in current_columns if col not in original_columns]\n",
    "\n",
    "print(\"Columns present in the final DataFrame:\\n\")\n",
    "print(current_columns)\n",
    "\n",
    "print(\"\\n Newly created columns:\")\n",
    "if new_columns:\n",
    "    print(new_columns)\n",
    "else:\n",
    "    print(\"No new columns were created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2577f529",
   "metadata": {},
   "source": [
    "### Part 2 - Clustering: Patient Risk Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25456a2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q2.1 Feature Preparation for Clustering (5 points)\n",
    "\n",
    "Before applying clustering algorithms, it is important to prepare the dataset by selecting relevant continuous features and standardizing them.\n",
    "\n",
    "**Tasks:**\n",
    "1. Select continuous features relevant to clustering: `age`, `SBI`, `CCS`, `ANRI`, and `stroke_risk_pct`.  \n",
    "2. Scale the selected features using `StandardScaler` so that each has mean = 0 and standard deviation = 1.  \n",
    "3. Display the shape of the scaled matrix and the first few transformed rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73ca34",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def prepare_clustering_features(df):\n",
    "    \"\"\"\n",
    "    Select and standardize relevant continuous features for clustering.\n",
    "    Returns the scaled feature matrix and its corresponding DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "    \n",
    "X_scaled = prepare_clustering_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea461e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q2.2 Optimal Number of Clusters (Elbow & Silhouette Methods) (20 points)\n",
    "\n",
    "To determine the appropriate number of clusters (**k**), we use two evaluation methods:\n",
    "\n",
    "- **Elbow Method:** observes the point where inertia (within-cluster variance) stops decreasing sharply.  \n",
    "- **Silhouette Score:** measures how well clusters are separated (higher = better).  \n",
    "\n",
    "**Tasks:**\n",
    "1. Run **K-Means** clustering for values of \\(k = 2\\) to \\(10\\).  \n",
    "2. Compute and store **Inertia** and **Silhouette Score** for each k.  \n",
    "3. Visualize both metrics to identify the k that balances compactness and separation.  \n",
    "4. Display the numeric summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f58cebb",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "def evaluate_kmeans_clusters(X_scaled, k_min=2, k_max=10):\n",
    "    \"\"\"\n",
    "    Evaluate optimal number of clusters (k) using Elbow and Silhouette methods.\n",
    "    Computes inertia and silhouette scores for k in [k_min, k_max].\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "opt_df = evaluate_kmeans_clusters(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e12cc8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q2.3 K-Means Clustering and Visualization (15 points)\n",
    "\n",
    "Using the results from the previous analysis, choose an appropriate number of clusters (**k**, typically 3–5).\n",
    "\n",
    "**K-Means** partitions patients into groups such that individuals in the same cluster are more similar to each other based on selected numeric features, while those in different clusters are more dissimilar.\n",
    "\n",
    "**Tasks:**\n",
    "1. Fit **K-Means** on the standardized feature matrix (`X_scaled`) with the chosen value of **k**.  \n",
    "2. Reduce the dataset to 2 dimensions using **PCA** for visualization.  \n",
    "3. Assign cluster labels to each patient and map descriptive names (e.g., *Low Risk*, *Moderate Risk*, *High Risk*).  \n",
    "4. Plot the clusters in 2D PCA space and highlight the cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ca291",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "def perform_kmeans_clustering(df, X_scaled, k=3):\n",
    "    \"\"\"\n",
    "    Fit K-Means clustering, project to 2D PCA space, and visualize clusters.\n",
    "    Returns DataFrame with cluster labels and centroids in PCA space.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "df, centroids_pca = perform_kmeans_clustering(df, X_scaled, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0126a5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q2.4 Cluster Profiling and Interpretation (10 points)\n",
    "\n",
    "Once the clusters are formed, it is important to interpret what each group represents.  \n",
    "We can do this by computing the **average feature values** for key indicators such as  \n",
    "`age`, `SBI`, `CCS`, `ANRI`, and `stroke_risk_pct`.\n",
    "\n",
    "**Tasks:**\n",
    "1. Compute the mean values of these features for each cluster.  \n",
    "2. Create a summary table showing the average characteristics per cluster.  \n",
    "3. Briefly interpret what each cluster might represent (e.g., *young–low-risk*, *older–high-burden*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893ac592",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def cluster_profiling(df):\n",
    "    \"\"\"\n",
    "    Summarize and display average feature values for each cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "cluster_profile = cluster_profiling(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716042e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q2.5 Cluster Risk Comparison (10 points)\n",
    "\n",
    "To evaluate the clinical significance of each cluster, we compare their overall stroke risk levels.  \n",
    "This helps determine whether certain clusters represent higher medical vulnerability.\n",
    "\n",
    "**Tasks:**\n",
    "1. For each cluster, compute:  \n",
    "   - Mean `stroke_risk_pct`  \n",
    "   - Proportion of patients where `at_risk = 1`  \n",
    "2. Create a summary table showing these statistics.  \n",
    "3. Visualize both metrics using side-by-side bar plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718862ac",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def cluster_risk_comparison(df):\n",
    "    \"\"\"\n",
    "    Compare average stroke risk and at-risk proportion across clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "cluster_risk = cluster_risk_comparison(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7db8c79",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q2.6 Gaussian Mixture Model (GMM) Comparison (25 points)\n",
    "\n",
    "While K-Means assigns each point to exactly one cluster,  \n",
    "a **Gaussian Mixture Model (GMM)** allows *probabilistic* membership, capturing overlap between patient groups.\n",
    "\n",
    "**Tasks:**\n",
    "1. Fit a GMM with the same number of clusters (**k**) used in K-Means.  \n",
    "2. Align GMM cluster labels with K-Means labels for consistency.  \n",
    "3. Compute the **Adjusted Rand Index (ARI)** to quantify agreement between the two models.  \n",
    "4. Visualize both K-Means and GMM results side by side in PCA space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411c3ce",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "def compare_kmeans_gmm(df, X_scaled, k=3):\n",
    "    \"\"\"\n",
    "    Compare K-Means and Gaussian Mixture Model (GMM) clustering.\n",
    "    Computes Adjusted Rand Index (ARI) and visualizes both cluster assignments.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "df, ari_aligned = compare_kmeans_gmm(df, X_scaled, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3957fa4f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "### Part 3 - Predictive Modeling: Stroke Risk Classification & Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3aecac",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q3.1 Feature Preparation (Symptoms Only) (5 points)\n",
    "\n",
    "In this step, we identify and prepare symptom-based features for later modeling.  \n",
    "These features capture physical and cardiovascular symptoms which may contribute to stroke risk.\n",
    "\n",
    "**Tasks:**\n",
    "1. Select all **symptom-related binary columns** from the dataset.  \n",
    "2. Initialize a preprocessing pipeline using `StandardScaler` for numeric scaling.  \n",
    "3. Display the list of selected features to confirm correct feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc20ba",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "def prepare_symptom_features(df):\n",
    "    \"\"\"\n",
    "    Select and scale symptom-related features for modeling.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "symptom_cols, preprocessor = prepare_symptom_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e58b8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q3.2 Regression – Predict Stroke Risk Percentage (20 points + bonus points)\n",
    "\n",
    "Train regression models using symptom-only features to predict the continuous variable `stroke_risk_pct`.  \n",
    "Evaluate them using **RMSE**, **MAE**, and **R²**.\n",
    "\n",
    "At the minimum use the following models:\n",
    "* k-nearest neighbors\n",
    "* random forest\n",
    "* linear regression\n",
    "\n",
    "5 bonus points for each additional model you use, for up to 2 additional models.\n",
    "\n",
    "Of course you are free to explore even more models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82619359",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "def train_regressors(df, symptom_cols, preprocessor):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple regression models for stroke risk prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "reg_df = train_regressors(df, symptom_cols, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e296e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q3.3 Best Regression Model – Feature Importance (20 points)\n",
    "\n",
    "For the regression model evaluated, visualize its **most important features** influencing the predicted stroke risk percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acab846",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_best_regressor(df, reg_df, symptom_cols, preprocessor):\n",
    "    \"\"\"\n",
    "    Display top features for the best-performing regression model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "best_reg_name = analyze_best_regressor(df, reg_df, symptom_cols, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0851e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q3.4 Classification – Predict Stroke Risk Category (20 points + bonus points)\n",
    "\n",
    "Using the same symptom-only features, train classification models to predict whether a patient is **at risk of stroke (1)** or **not at risk (0)**.  \n",
    "Evaluate using **AUC**, **Accuracy**, **F1**, and **Balanced Accuracy**.\n",
    "\n",
    "Use the following models:\n",
    "* Random Forest\n",
    "* k-nearest neighbors\n",
    "* logistic regression\n",
    "\n",
    "5 bonus points for each additional model you use, for up to 2 additional models.\n",
    "\n",
    "Of course you are free to explore even more models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acbab5",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "def train_classifiers(df, symptom_cols, preprocessor):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple classification models to predict at-risk status.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "cls_df = train_classifiers(df, symptom_cols, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0876ca71",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q3.5 Best Classification Model – Confusion Matrix & Feature Importance (15 points)\n",
    "\n",
    "For the model evaluated, display its **confusion matrix** and visualize its **most important features** influencing the stroke-risk classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a483c",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "def analyze_best_classifier(df, cls_df, symptom_cols, preprocessor):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the best classification model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "best_cls_name = analyze_best_classifier(df, cls_df, symptom_cols, preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24254a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Q3.6 Kaggle Submission (15 points + bonus points)\n",
    "\n",
    "In the Kaggle competition, you will generate predictions for an unseen test dataset.  \n",
    "Since the hidden test file is **not provided**, we will simulate this process using our model’s predictions on the validation (test) split.\n",
    "\n",
    "**Tasks:**\n",
    "1. Use your **best classification model** to predict stroke risk (`at_risk`) on the test set.  \n",
    "2. Create a **submission DataFrame** with columns:  \n",
    "   - `id` (sequential from 1 to n)  \n",
    "   - `at_risk` (predicted 0 or 1)  \n",
    "3. Save it as `sample_submission.csv` to simulate a Kaggle submission file.\n",
    "\n",
    "The top 10 finishers get an additional 10 bonus points.\n",
    "\n",
    "The 11-20 finishers get an additional 5 bonus points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a70cb",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def create_kaggle_submission_simulated(df, symptom_cols, preprocessor, cls_df, filename=\"kaggle_submission.csv\"):\n",
    "    \"\"\"\n",
    "    Create Kaggle-style submission using the best classification model from cls_df.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "\n",
    "submission = create_kaggle_submission_simulated(df, symptom_cols, preprocessor, cls_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e5fe40",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
